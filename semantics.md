

#  语义

## 语义相关的英文词汇
semantics ： 语义学

semantic atom  : 语义原子

semantic componentlal analysis : 语义成分分析

semantic field : 语义场

convolutional neural network ：卷积神经网络

## 语义的范畴

## 语义分析有哪些
###文本语义分析


## 语义如何表达
#### 1. 传统向量空间模型
(1) 主要步骤
* 将文本的基本语言单位（字、词、词组、短语）抽取，组成特征项，用tn表示
* 将tn按在文本中的重要性给出权重wn
* 将文本抽象为（t1,w1,t2,w2,……,tn,wn）简化为（w1,w2,……,wn）即为文本的向量 空间模型。

(2) 权值wn计算
* 布尔：取值1/0表示该特征是否在文本中出现；
* 词频（TF）：wn用特征在文档中出现的频数表示；
* 文档频率(IDF)：通过该特征词在所有文档中的出现频率来分配权重；
* TF/IDF权值：将前两种词的特征相乘得到权重值。

传统向量空间模型有明显的缺点：
* 基于关键字的文档处理方法，依据的是词频信息，两个文档的相似度取决于共同词汇的数量，无法分辨自然语言的语义模糊性。
* 假设词与词之间是相互独立的，一个关键字唯一代表一个概念或语义单元，而实际情况是文档存在很多的一词多义和同义词现象，因此这种假设很难满足实际情况。
* 文档中词与词往往存在一定的关联性，信息检索的本质就是语义的检索，孤立的用关键字来表示文档内容，通过简单的词汇模式匹配进行检索，忽视上下文语境的影响作用，会影响到信息检索的结果的查准率和查全率。

#### 2.	one-hot词向量
NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。

举个栗子，
“话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...]

“麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]

每个词都是茫茫 0 海中的一个 1。

这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 ID。比如刚才的例子中，话筒记为 3，麦克记为 8（假设从 0 开始记）。如果要编程实现的话，用 Hash 表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。

#### 3. Distributed representation 词向量
现在俗称的词向量主要指Distributioned Representation，是是一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, ...]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的。

Distributed representation 最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。

参考：

* 文本表示+向量表示 http://blog.sina.com.cn/s/blog_795b865e0102v984.html
* Deep Learning in NLP （一）词向量和语言模型 http://blog.csdn.net/zhoubl668/article/details/23271225
