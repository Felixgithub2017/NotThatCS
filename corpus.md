# 语料库

建立样本库是数据挖掘、人工智能研究不可缺少的基础工作。

语料库是指以文本类型为样本的数据集。有的语料库重在数量，有的则关注加工。

推荐资料：

图片分类的TED(李飞飞) https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?language=zh-cn

儿童习得语言的资料TED记录：单词的诞生http://open.163.com/movie/2011/7/E/J/M77TUGO7R_M77TUSJEJ.html

## 语料库列表
- 语料库在线 http://www.cncorpus.org/
- 现代汉语语料库 http://ccl.pku.edu.cn/corpus.asp?item=1
- 古代汉语语料库 http://ccl.pku.edu.cn/corpus.asp?item=2
- 汉英双语语料库 http://ccl.pku.edu.cn/corpus.asp?item=3
- HSK动态作文语料库 http://202.112.195.192:8060/hsk/login.asp
- 北京口语语料查询系统 http://www.blcu.edu.cn/yys/6_beijing/6_beijing_chaxun.asp
- 现代汉语平衡语料库 http://rocling.iis.sinica.edu.tw/new/20corpus.htm
- LIVAC共時語料庫 http://www.livac.org/index.php
- 兰开斯特汉语语料库 http://ling.cass.cn/dangdai/LCMC/LCMC.htm
- 洛杉矶加州大学汉语语料库 http://www.lancs.ac.uk/fass/projects/corpus/UCLA/
- 中文新闻分类语料库 http://www.nlpir.org/?action-viewnews-itemid-145
- NLPIR 500万条twitter内容语料库 http://www.nlpir.org/?action-viewnews-itemid-263
- NLPIR微博博主语料库100万条 http://www.nlpir.org/?action-viewnews-itemid-232
- 現代漢語語料庫詞頻統計 http://elearning.ling.sinica.edu.tw/CWordfreq.html
- 欢迎关注新浪微博【对外汉语北京】
- 中文句結構樹資料庫 http://turing.iis.sinica.edu.tw/treesearch/
- 搜狗文本分类语料库 http://www.sogou.com/labs/dl/c.html
- 哈工大信息检索研究室对外共享语料库 http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm
- 传媒大学文本语料库 http://ling.cuc.edu.cn/RawPub/
- 词语研究资源库 对外汉语北京 http://ling.cuc.edu.cn/newword/web/index.asp
- BFSU CQPweb多语言在线语料库检索平台 http://www.iresearch.ac.cn/paper/detail.php?ItemID=6358
- 英汉双语平行语料库 http://www.luweixmu.com/ec-corpus/
- babel 汉英平行语料库 http://icl.pku.edu.cn/icl_groups/parallel/default.htm
- 中国法律法规汉英平行语料库（大陆） http://corpus.zscas.edu.cn/lawcorpus1/index.asp
- 国家语言资源监测与研究中心 http://www.clr.org.cn/

## 样本收集方法

## 语料库加工方法

### 词语切分

#### 汉语自动分词技术

1. 概念
通过计算机把组成汉语文本的字串自动转换为词串的过程
2. 目的
    - TTS或语音合成： 只有正确切词，才能知道正确的发音
    - 信息检索： 分词有助于提高信息检索的准确率
    - 词语的计量分析：词频统计等
    - 深层汉语分析的基础：句法分析、语义分析等
3. 基本方法
    - 最大匹配法(MM)
        - 正向最大匹配法(MM)
        - 逆向最大匹配法(RMM)
4. 评价标准
    - 准确率(precision)
      准确率（P）＝切分结果中正确分词数/切分结果中所有分词数*100%
    - 召回率(recall)
      召回率（R）＝切分结果中正确分词数/标准答案中所有分词数*100%
    - F-评价(F-measure综合准确率和召回率的评价指标)
      F-指标＝2PR/(P+R)
5. 关键问题
    - 切分歧义（消解）：一个字串有不止一种切分结果
    - 未登录词识别：专有名词、新词

### 词性标注

## 其他相关内容
### 构建语料库的原则
语料库应该具有代表性、结构性、平衡性、规模需求并制定语料的元数据规范，各个原则具体介绍如下：

* 代表性：在应用领域中，不是根据量而划分是否是语料库，而是在一定的抽样框架范围内采集而来的，并且在特定的抽样框架内做到代表性和普遍性。
* 结构性：有目的的收集语料的集合，必须以电子形式存在，计算机可读的语料集合结构性体现在语料库中语料记录的代码，元数据项、数据类型、数据宽度、取值范围、完整性约束。
* 平衡性：主要体现在平缓因子：学科、年代、文体、地域、登载语料的媒体、使用者的年龄、性别、文化背景、阅历、预料用途（私信/广告等），根据实际情况选择其中一个或者几个重要的指标作为平衡因子，最常见的平衡因子有学科、年代、文体、地域等。
* 规模性：大规模的语料对语言研究特别是对自然语言研究处理很有用的，但是随着语料库的增大，垃圾语料越来越多，语料达到一定规模以后，语料库功能不能随之增长，语料库规模应根据实际情况而定。
* 元数据：元数据对于研究语料库有着重要的意义，我们可以通过元数据了解语料的时间、地域、作者、文本信息等；还可以构建不同的子语料库；除此外，还可以对不同的子语料对比；另外还可以记录语料知识版权、加工信息、管理信息等。

参考：[【NLP】大数据之行，始于足下：谈谈语料库知多少](http://www.cnblogs.com/baiboy/p/ylk.html)

### NLTK库及其包含的语料库介绍

NLTK是斯坦福大学开发的处理自然语言的python库，主要包含如下功能：

* 语料库：提供了经典书籍，词典，演讲，网络语言，论坛等各种语料库；
* 断句与分词：可以方便地对文章，段落进行分词；
* 词频统计：计算句子或者文章中每个单词的频率；
* 同义词与词态：单词的同义词「WordNet」和单词词态「过去式，进行时等」的还原；
* 词性的标注：动词，名词，形容词和副词等的标注；
* 分类算法：例如常见的信息熵，朴素贝叶斯算法，最大信息熵模型等；

许多英文自然语言处理的研究都建立在NLTK提供的语料库上，其中比较常用有以下几种：

* `gutenberg`：古登堡语料库，古登堡计划致力于将文化作品的数字化和归档，古登堡语料库选择了 部分文本，包含了一百七十万字。
* `webtext`：网络文本语料库，网络和聊天文本
* `brown`：布朗语料库，按照文本分类好的500个不同来源的文本
* `reuters`：路透社语料库，1万多个新闻文档
* `inaugural`：就职演说语料库，55个总统的演说

安装`NLTK库`后利用`from nltk.corpus import <语料名>`即可导入相应的语料库。此外，还有一些仅是词或短语以及一些相关信息的集合，叫做词典资源。

* 词汇列表语料库：`nltk.corpus.words.words()`，所有英文单词，可以用来识别语法错误
* 停用词语料库：`nltk.corpus.stopwords.words()`，用来识别那些最频繁出现的信息量较小的词
* 发音词典：`nltk.corpus.cmudict.dict()`，用来输出每个英文单词的发音
* 比较词表：`nltk.corpus.swadesh()`，多种语言核心200多个词的对照，可以作为语言翻译的基础
* 同义词集：WordNet，面向语义的英语词典，由同义词集组成，并组织成一个网络

参考：
[NLTK笔记：简介与环境搭建](http://blog.ourren.com/2015/02/05/nltk_note_environment_install/)

[自己动手做聊天机器人 三-语料与词汇资源](http://www.shareditor.com/blogshow/?blogId=65)


### 维基百科中文语料库

维基百科的英文语料库因其高质量早已被广泛运用，其提供了中文版的语料库后，
逐渐在中文自然语言处理中广泛使用，其资源获取非常方便，可直接在[Wiki Dump](https://dumps.wikimedia.org/zhwiki/)上下载，其更新速度也很快，最新一次备份时间为2016年10月1日。虽然中文维基百科中的文字均是繁体，但是在经过简单的繁体到简体转换就能方便地使用，对该语料库文档的解析有非常多的成熟工具，直接使用开源工具即可完成正文的提取。

对于该语料库的使用方式和应用场景也在多篇博文中提到：

* [中英文维基百科语料上的Word2Vec实验](http://www.52nlp.cn/中英文维基百科语料上的word2vec实验)
* [维基百科简体中文语料的获取](http://licstar.net/archives/262)
* [用wiki百科中文语料训练word2vec模型](http://blog.csdn.net/hereiskxm/article/details/49664845)

参考：
[自然语言处理之语料库资源](http://blog.just4fun.site/NLP-corpus.html)

### 搜狗实验室数据资源 （http://www.sogou.com/labs/）  
	1. 包含了5大类13种数据资源  
		1.1 评测集合   
			1.1.1 搜索结果评价   
				简介：判断搜索结果与查询的相关性，是否符合搜索意图。  
				数据：完整版4326条，数据格式“查询词\t相关的URL\t查询类别”。  
				相关任务：基于互联网语料的信息检索。  
				相关技术：2.1。   
			1.1.2 话题跟踪及检测评价   
				简介：评测新闻话题跟踪及检测效果。  
				数据：完整版953条，数据格式“URL\t话题名称”。  
				相关任务：文本分类。  
				相关技术：2.2  
			1.1.3 文本分类评价   
				简介：评估文本分类结果的正确性。    
				数据：94条，数据格式“URL前缀\t对应类别标记”。  
				相关任务：文本分类。  
		1.2 语料数据   
			1.2.1 互联网语料库  
				简介：来自互联网各种类型的1.3亿个原始网页。  
				数据：完整版1TB，迷你版10个页面数据。  
				相关任务：相关性排序，文本分类，新词发现，机器翻译，分词。  
				相关技术：2.3，2.4      
			1.2.2 链接关系库  
				简介：包括对应互联网语料库内文档的链接关系列表。  
				数据：完整版90GB，迷你版1000条URL对照表和1000条链接关系。  
				相关任务：相关行排序，链接分析，反垃圾。  
				相关技术：2.4  
			1.2.3 SogouRank库  
				简介：互联网语料库中各页面的重要程度评级。  
				数据：完整版90GB，迷你版1001条，数据格式“URL\tSougouRank”。  
				相关任务：相关性排序。  
			1.2.4 用户查询日志  
				简介：搜索引擎部分网页查询需求及用户点击情况的网页查询日志数据。  
				数据：完整版1.9GB，迷你版10000条，数据格式“访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL”。  
				相关任务：相关行排序，用户兴趣挖掘，查询扩展，新词发现。  
		1.3 新闻数据  
			1.3.1 全网新闻数据  
				简介：来自5个新闻站点共83个频道的新闻数据，提供URL和正文信息。  
				数据：完整版1.02GB，迷你版200条新闻数据。  
				相关任务：文本分类，事件检测跟踪，新词发现，命名实体识别，自动摘要。  
				相关技术：2.2  
			1.3.2 搜狐新闻数据  
				简介：来自搜狐新闻共18个频道的新闻数据，提供URL和正文信息。  
				数据：完整版65GB，迷你版200条新闻数据，论文（见2.2）版953条。 
				相关任务：文本分类，事件检测跟踪，新词发现，命名实体识别，自动摘要。  
				相关技术：2.2  
		1.4 图片数据  
			1.4.1 互联网图片库  
				简介：来自sougou图片搜索索引的280多万张抓取图片及标注数据集合。    
				数据：完整版269GB。  
				相关任务：基于文本/内容的图片检索。  
			1.4.2 互联网图片库2.0  
				简介：1000万张互联网图片和相关文本信息，以及识图搜索结果的人工标注集合。    
				数据：完整版635GB。  
				相关任务：基于内容的图片检索。  
		1.5 自然语言处理相关数据   
			1.5.1 互联网词库  
				简介：基于互联网语料环境的高频词对应的词频、词性信息。      
				数据：完整版1.3MB，共157202个词。  
				相关任务：中文词性标注，词频分析。  
			1.5.2 中文词语搭配库  
				简介：基于互联网语料的字词搭配关系统计。        
				数据：完整版共18399496组，迷你版共100000组。  
				相关任务：中文输入法，文字到语音转化，语音识别。  
				相关技术：2.5，2.6  
	2. 提出了一些相关技术并进行了实验  
		2.1 基于点击数据分析，自动对搜索结果进行评估。(http://www.sogou.com/labs/paper/Automatic_Search_Engine_Performance_Evaluation_with_Click-through_Data_Analysis.pdf)   
			- 将搜索分成三种情况：找特定网址、找信息、处理事务。“找特定网址”具有明确的目标特征。因此这里仅考虑这类搜索情况。  
			- 通过统计搜索词q与搜索结果中点击量最多的链接r，统计（q，r），并在下次给出搜索结果时让r更靠前，如此不断验证。  
			- 从2006年6月至2007年1月，对sougou.com的检索和点击数据进行统计。  
			- 准确率为97%左右。错误结果中，一些网址是正确网址的子网站。例如，搜索163通常会定位到163邮箱mail.163.com，而非www.163.com。        
		2.2 网络环境下自动进行在线新闻事件的生成。(http://www.sogou.com/labs/paper/Automatic_Online_News_Issue_Construction_in_Web_Environment.pdf)    
			- 方法分三步：  
				1）话题检测，将新出现的文本内容聚类成候选话题。  
				2）话题对比，候选话题与既有的话题比较，并入既有话题或称为新的话题。  
				3）根据相关话题生成新闻事件。  
			- 具体做法：   
				1）预处理：提取页面内容，分句，除去无用句，对单词进行标记（中文进行词汇划分），词性标注，识别命名实体，删除“应删除词”（例如“的”），最终每篇文章生成一个词向量。  
				2）计算t时刻词w的“词频”，每篇文章表示表示为t时刻的一个n维向量，使用增量TF-IWF模型计算各维度权重weight t(d,w)并归一化处理。  
				3）使用余弦相似度算法计算两篇文章间的相似度。  
				4）使用算术平均加权配对组（UPGMA）的聚类方法，将新文章聚类至候选话题中。    
				5）对候选话题与既有话题相比较，并入并更新既有话题，或成为新的话题。  
				6）类似地将各话题聚类到新闻事件中，并不断更新新闻事件或生成新的新闻事件。    
				7）自动对各新闻事件加入网络上的相关博文、评论和图片、音频、视频等。 
			- 实验数据：  
				数据集1：含350个新闻页面，87个话题。2007年3月到4月，均关于搜索引擎公司。中文新闻网站，包括新闻报道和新闻回顾。最多的话题包含20个文章，最少1个。  
				数据集2：含953个新闻页面，108个话题。2007年4月22日的搜狐体育新闻频道。最大话题有151个文章，最少1个。  
				数据集3：含24872个新闻页面，选自多个中文新闻网站和1339篇博文和评论文章。  
			- 实验结果：  
				1）聚类方法的选择：direct-I2-IDF、direct-I2-IWF、rbr-H2-IDF、rbr-H2-IWF、graph-jacc-IDF、graph-jacc-IWF、agglo-upgma-IDF、agglo-upgma-IWF这八个算法中，“aggloupgma-IWF”算法更优。  
				2）IWF与IDF比较：IWF模型的效果更平滑更优。  
				3）冗余句子的去除（RSR）：新闻中冗余句子较少，RSR有效果但不明显。  
				4）标题的使用及权重：加标题表现更好；仅短文加标题比全部加标题更好；标题权重比正文相对更大表现更好。    
		2.3 使用与查询无关的特征对网络信息检索数据进行清洗。(http://www.sogou.com/labs/paper/Data_Cleansing_for_Web_Information_Retrieval_using_Query_Independent_Features.pdf)  
			- 若使用普通的链接分析方法，基于网页的被点击概率，而非页面的有用度。因此使用与查询无关的特征。  
			- 结合利用了链接分析和页面布局分析，进行全局规模的数据清洗。  
			- 使用朴素贝叶斯学习算法，在低维度实例空间上高效实用，且不需要原有数据集的先验知识。  
			- 将5个特征：文字长度，链接文本长度，搜索排名值，导入链接数量和导出链接数量综合应用，比仅使用单独一个特征效果更好。  
			- 清洗后选出的高质量页面占全部的52%（.GOV数据集）和5%（sogou数据集），召回率>90%。即牺牲了10%的正确搜索结果，大大节省了存储空间。  
			- 同时能够消除30%的垃圾页面和15%的低质量页面。  
		2.4 一种基于链接分析的垃圾页面检测算法。(http://www.sogou.com/labs/paper/R-SpamRank_A_Spam_Detection_Algorithm_Based_on_Link_Analysis.pdf)  
			- 一些人试图误导搜索引擎以提升网页的搜索排名，方法主要是基于内容和基于链接两种。这里提出一种半自动的检测基于链接的垃圾网页的方法。  
			- 使用人工识别的垃圾页面黑名单作为种子，设置高RSR值。根据链接到本页面的情况，反向传播RSR值，并不断迭代直到各页面值稳定。  
			- 测试数据为sogou.com的500万个网页，迭代50次。人工分析后，不能打开的页面赋值0，好的页面赋值1，半垃圾页面赋值2，纯垃圾页面赋值3。  
			- 将结果中RSR值最高的1万个页面中的前100个和最后100个取出做人工检查。发现99%是垃圾页面。证明算法有效。  
			- 发现垃圾页面集中在两个域名，说明算法能够检测链接工厂。  
			- 做域名清理，即删除3个链接工厂下的所有页面后再分析。剩下的178个页面中仍有87.1%的垃圾页面。  
			- 前5大链接工厂产生了99.1%的垃圾页面。  
		2.5 基于相对条件熵的搭配抽取方法。(http://www.sogou.com/labs/paper/Wangdaliang_JoBUPT_07.pdf)   
			- 在自然语言处理中，研究搭配组词项之间的内在倾向性。提出使用相对条件熵比传统的使用互信息评价二元相关性更优。  
			- 词语在语料库中出现越频繁，越容易失去倾向的特性。  
			- 绝大多数次的左搭配力大于自己的右搭配力。  
			- 左搭配倾向强的多是定语；右搭配倾向抢的多是宾语。  
			- 自然文本的搭配抽取方法：  
				1）预处理：提取文本，分词，词性标注，停用词过滤，词频过滤。    
				2）数据统计：利用词性过滤模板、滑动窗口生成搭配候选二元组统计矩阵，同时统计二元同现次数、构成次的词频以及样本总词频。  
				3）搭配抽取：计算候选二元组左、右搭配倾向强度以及搭配整合强度，依据阈值输出搭配抽取结果。  
			- 对sogou.com的1亿多个中文页面语料库数据进行实验。预处理得搭配候选二元组35万个，自动获取搭配词对3.5万个。人工验证得效果比互信息法好。  
		2.6 多策略融合的搭配抽取方法。(http://www.sogou.com/labs/paper/Wangdaliang_JoTHU_08.pdf)  
			- 搭配抽取中，需要识别频繁二元组和稀疏二元组，而排除无关二元组。  
			- 互信息法可作为二元组无关性的度量方法，用于排除大部分无关二元组。  
			- 卡方检验法比t检验法更适合于刻画二元组的相关性，且能很好识别频繁二元组，但对稀疏二元组不行。  
			- 对数似然比检验法可用于识别稀疏二元组。  
			- 对sogou.com的1亿多个中文页面语料库数据进行实验。预处理得搭配候选二元组35万个，自动获取搭配词对5万个。人工验证得多策略融合法效果更好。  
