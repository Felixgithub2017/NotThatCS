

#  语义

## 语义相关的英文词汇
semantics ： 语义学

semantic atom  : 语义原子

semantic componentlal analysis : 语义成分分析

semantic field : 语义场

convolutional neural network ：卷积神经网络

## 语义的范畴

## 语义分析有哪些
### 文本语义分析
文本处理的基本方法构成了语义分析的基础

一个文本串除了分词，还需要做词性标注，命名实体识别，新词发现等。通常有两种方案，一种是pipeline approaches，就是先分词，再做词性标注；另一种是joint approaches，就是把这些任务用一个模型来完成。
    
#### 文本基本处理

##### 1.1中文分词

* 基于字符串匹配的分词方法。此方法按照不同的扫描方式，逐个查找词库进行分词。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分(即最短路径)；总之就是各种不同的启发规则。
* 全切分方法。它首先切分出与词库匹配的所有可能的词，再运用统计语言模型决定最优的切分结果。它的优点在于可以解决分词中的歧义问题。对于文本串“南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型找到最优路径，最后可能还需要命名实体识别。下图中“南京市 长江 大桥”的语言模型得分，即P(南京市，长江，大桥)最高，则为最优切分。
* 由字构词的分词方法。可以理解为字的分类问题，也就是自然语言处理中的sequence labeling问题，通常做法里利用HMM，MAXENT，MEMM，CRF等预测文本串每个字的tag，譬如B，E，I，S，这四个tag分别表示：beginning, inside, ending, single，也就是一个词的开始，中间，结束，以及单个字的词。 例如“南京市长江大桥”的标注结果可能为：“南(B)京(I)市(E)长(B)江(E)大(B)桥(E)”。由于CRF既可以像最大熵模型一样加各种领域feature，又避免了HMM的齐次马尔科夫假设，所以基于CRF的分词目前是效果最好的。除了HMM，CRF等模型，分词也可以基于深度学习方法来做。

参考文献：[斯坦福课程-语言模型] (http://52opencourse.com/111/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88language-modeling%EF%BC%89)

##### 1.2Term Weighting
对文本分词后，接下来需要对分词后的每个term计算一个权重，重要的term应该给与更高的权重。Term weighting在文本检索，文本相关性，核心词提取等任务中都有重要作用。

利用有监督机器学习方法来预测weight。这里类似于机器学习的分类任务，对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。既然是有监督学习，那么就需要训练数据。如果采用人工标注的话，极大耗费人力，所以可以采用训练数据自提取的方法，利用程序从搜索日志里自动挖掘。从海量日志数据里提取隐含的用户对于term重要性的标注，得到的训练数据将综合亿级用户的“标注结果”，覆盖面更广，且来自于真实搜索数据，训练结果与标注的目标集分布接近，训练数据更精确。

* 从搜索session数据里提取训练数据，用户在一个检索会话中的检索核心意图是不变的，提取出核心意图所对应的term，其重要性就高。
* 从历史短串关系资源库里提取训练数据，短串扩展关系中，一个term出现的次数越多，则越重要。
* 从搜索广告点击日志里提取训练数据，query与bidword共有term的点击率越高，它在query中的重要程度就越高。

参考文献：[Deep learning for Chinese word segmentation and POS tagging]( http://www.aclweb.org/anthology/D13-1061)

[Max-margin tensor neural network for chinese word segmentation]( http://aclweb.org/anthology/P14-1028)

[Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)

[Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)

[Chinese Segmentation and New Word Detection using Conditional Random Fields](http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1091&context=cs_faculty_pubs)

### 对于不同层次的处理对象，都需要语义分析：
 - 对于“词”：
   - 词义消歧：判断一个词是不是多义词，对多义词不同含义的表示和判别，结合上下文判断多义词在当前环境下的意思。（方法：根据词典释义、基于语义定义、利用语料库）
 - 对于“句子”：
   - 语义角色标注：识别句子中各词的作用，特别是围绕句中的“动词”识别句中相关成分（谓词-论元结构）。（方法：利用语料库）
 - 对于“篇章”：
   - 指代消解：确定文章中的代词是指的哪个名词。
   - 篇章语义整合：把篇章片段的语义正确整合，得到篇章的全局意图。（方法：基于句法，利用语料库）
 - 可以看到语义分析常常利用语料库（例如pdtb这样的带有标注的熟语料库），使用包括统计、机器学习等方法。

### 词语语义分析

词语语义分析研究是指确定词语意义，衡量词之间的语义相似度（相关度）。词语的相似性和相关性既有联系又有区别。相似性指词语间的可替代性，而相关性指词语的相关程度。例如：“爱情”和“浪漫”是 两个语义相关的词语，但它们的语义不可替代。“橙子”和“橘子”是语义相似的词语，它们指同类水果，同时也有语义关联。根据研究方法的不同，国内外已有的词语语义相关度或相似度的研究大体可分为两大类：基于词语语义知识规则（如语义词典、知识库、概念本体库等）的词语语义分析和基于统计的词语语义分析。

#### 基于知识规则的词语语义相似（相关）分析

基于知识规则的词语语义分析是一种基于语言学的词汇语义分析的理性主义方法，它利用词语语义知识库中定义好的概念及其之间的上下文关系等逻辑关系，通过计算两个概念在概念体系中的距离来衡量词 语间的语义相似或相关度。

#### 基于统计的词语语义分析

基于统计的词语语义相似（相关）度分析方法是一种经验主义方法，它以代数理论、概率论和统计论等数学理论为基础，建立在可观察的语言事实上。该方法认为：两个词语语义相似或相关，当且仅当它们处于相似或相关的上下文环境中。通过对大规模语料库的统计，该方法将词语的上下文信息作为语义相似或相关分析的主要参照依据。

参考文献：秦春秀, 祝婷, 赵捧未, 等. 自然语言语义分析研究进展[J]. 图书情报工作, 2014, 58(22): 130-137.

### 图像语义分析
图像理解就是对图像的语义解释.它是以图像为对象，知识为核心，研究图像中何位置有何目标、目标场景之间的相互关系、图像是何场景以及如何应用场景的一门科学。图像理解输入的是数据，输出的是知识，属于图像研究领域的高层内容。语义作为知识信息的基本描述载体，能将完整的图像内容转换成可直观理解的类文本语言表达，在图像理解中起着至关重要的作用。

图像理解中的语义分析在应用领域的潜力是巨大的。图像中丰富的语义知识可提供较精确的图像搜索引擎，生成智能的数字图像相册和虚拟世界中的视觉场景描述。同时，在图像理解本体的研究中，可有效形成 “数据-知识”的相互驱动体系，包含有意义的上下文信息和层状结构信息，能更快速、更准确地识别和检测出场景中的特定目标 (如识别出场景中的 “显示器 ”, 根据场景语义知识可自动识别附近的 “键盘” )。

尽管语义分析在图像理解中处于非常重要的位置，但传统的图像分析方法基本上全部回避了语义问题，仅针对纯粹的图像数据进行分析。究其原因主要集中于两方面：1)图像的视觉表达和语义之间很难建立合理关联 , 描述实体间产生巨大的语义鸿沟;  2)语义本身具有表达的多义性和不确定性。目前，越来越多的研究已开始关注上述 “瓶颈 ”, 并致力于有效模型和方法以实现图像理解中的语义表达。

解决图像理解中的语义鸿沟需要建立图像和文本之间的对应关系，解决的思路可大致分为三类。第一条思路侧重于图像本身的研究，通过构建和图像内容相一致的模型或方法，将语义隐式地融入其中，建立 “文本→图像”的有向联系，核心在于如何将语义融于模型和方法中。采用此策略形成的研究成果多集中于生成方式和判别方式中。第二条思路从语义本身的句 法表达和结构关系入手，分析其组成及相互关系，通过建立与之类似的图像视觉元素结构表达，将语义描述和分析方法显式地植入包含句法关系的视觉图中，建立“图像→文本”的有向联系。核心在于如何构建符合语义规则的视觉关系图。第三条思路面向应用,以基于内容的图像检索为核心,增加语义词汇规模，构建多语义多用户多进程的图像检索查询系统。

图像内容的语义分析借鉴文本分析策略。首先需要构建与之相对应的对象，整幅图像对应整篇 文档，而文档中的词汇也需要对应相应的视觉词汇。视觉词汇的获取一般通过对图像信息的显著性分析提取图像的低层特征，低层特征大多从图像数据获取，包括简单的点线面特征和一些特殊的复杂特征再由鲁棒的特征表达方式生成合适的视觉词汇,视觉词汇一般具有高重用性和若干不变特性。

参考文献：
《图像语义分析与理解综述》
http://xueshu.baidu.com/s?wd=paperuri%3A%281a7e0a172268dc92eb03e4afcd5958f1%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Fwww.cnki.com.cn%2FArticle%2FCJFDTotal-MSSB201002010.htm&ie=utf-8&sc_us=8690416712663863870

## 语义如何表达

### 文本表示
#### 1. 传统向量空间模型
(1) 主要步骤
* 将文本的基本语言单位（字、词、词组、短语）抽取，组成特征项，用tn表示
* 将tn按在文本中的重要性给出权重wn
* 将文本抽象为（t1,w1,t2,w2,……,tn,wn）简化为（w1,w2,……,wn）即为文本的向量 空间模型。

(2) 权值wn计算
* 布尔：取值1/0表示该特征是否在文本中出现；
* 词频（TF）：wn用特征在文档中出现的频数表示；
* 文档频率(IDF)：通过该特征词在所有文档中的出现频率来分配权重；
* TF/IDF权值：将前两种词的特征相乘得到权重值。

传统向量空间模型有明显的缺点：
* 基于关键字的文档处理方法，依据的是词频信息，两个文档的相似度取决于共同词汇的数量，无法分辨自然语言的语义模糊性。
* 假设词与词之间是相互独立的，一个关键字唯一代表一个概念或语义单元，而实际情况是文档存在很多的一词多义和同义词现象，因此这种假设很难满足实际情况。
* 文档中词与词往往存在一定的关联性，信息检索的本质就是语义的检索，孤立的用关键字来表示文档内容，通过简单的词汇模式匹配进行检索，忽视上下文语境的影响作用，会影响到信息检索的结果的查准率和查全率。

#### 2.	one-hot词向量
NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。

举个栗子，
“话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...]

“麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]

每个词都是茫茫 0 海中的一个 1。

这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 ID。比如刚才的例子中，话筒记为 3，麦克记为 8（假设从 0 开始记）。如果要编程实现的话，用 Hash 表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。

#### 3. Distributed representation 词向量
现在俗称的词向量主要指Distributioned Representation，是是一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, ...]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的。

Distributed representation 最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。

Word2vec是比较常用的一种，其基本思想是通过训练将每个词映射成 K 维实数向量（K 一般为模型中的超参数），通过词之间的距离（比如 cosine 相似度、欧氏距离等）来判断它们之间的语义相似度.其采用一个三层的神经网络 ，输入层-隐层-输出层。有个核心的技术是 根据词频用Huffman编码 ，使得所有词频相似的词隐藏层激活的内容基本一致，出现频率越高的词语，他们激活的隐藏层数目越少，这样有效的降低了计算的复杂度。而Word2vec大受欢迎的一个原因正是其高效性，Mikolov 在论文中指出，一个优化的单机版本一天可训练上千亿词。这个三层神经网络本身是对语言模型进行建模，但也同时获得一种单词在向量空间上的表示 ，而这个副作用才是Word2vec的真正目标。
  
Word2Vec实际上是两种不同的方法：Continuous Bag of Words (CBOW) 和 Skip-gram。CBOW的目标是根据上下文来预测当前词语的概率。Skip-gram刚好相反：根据当前词语来预测上下文的概率。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机 N 维向量。经过训练之后，该算法利用 CBOW 或者 Skip-gram 的方法获得了每个单词的最优向量。

#### 4. 文档向量
即使上述模型对词向量进行平均处理，我们仍然忽略了单词之间的排列顺序对情感分析的影响。即上述的word2vec只是基于词的维度进行"语义分析"的，而并不具有上下文的"语义分析"能力。作为一个处理可变长度文本的总结性方法，Quoc Le 和 Tomas Mikolov 提出了 Doc2Vec方法。除了增加一个段落向量以外，这个方法几乎等同于 Word2Vec。和 Word2Vec 一样，该模型也存在两种方法：Distributed Memory(DM) 和 Distributed Bag of Words(DBOW)。DM 试图在给定上下文和段落向量的情况下预测单词的概率。在一个句子或者文档的训练过程中，段落 ID 保持不变，共享着同一个段落向量。DBOW 则在仅给定段落向量的情况下预测段落中一组随机单词的概率。

word2vec&doc2vec具体训练过程参考：[文本深度表示模型——word2vec&doc2vec词向量模型](http://www.wtoutiao.com/p/238FL4B.html)

其它参考文献：

* 文本表示+向量表示 http://blog.sina.com.cn/s/blog_795b865e0102v984.html
* Deep Learning in NLP （一）词向量和语言模型 http://blog.csdn.net/zhoubl668/article/details/23271225 

#语言模型

语言模型是用来计算一个句子产生概率的概率模型，即P(w_1,w_2,w_3…w_m)，m表示词的总个数。根据贝叶斯公式：P(w_1,w_2,w_3 … w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2) … P(w_m|w_1,w_2 … w_{m-1})。

最简单的语言模型是N-Gram，它利用马尔科夫假设，认为句子中每个单词只与其前n–1个单词有关，即假设产生w_m这个词的条件概率只依赖于前n–1个词，则有P(w_m|w_1,w_2…w_{m-1}) = P(w_m|w_{m-n+1},w_{m-n+2} … w_{m-1})。其中n越大，模型可区别性越强，n越小，模型可靠性越高。

N-Gram语言模型简单有效，但是它只考虑了词的位置关系，没有考虑词之间的相似度，词语法和词语义，并且还存在数据稀疏的问题，所以后来，又逐渐提出更多的语言模型，例如Class-based ngram model，topic-based ngram model，cache-based ngram model，skipping ngram model，指数语言模型（最大熵模型，条件随机域模型）等。

最近，随着深度学习的兴起，神经网络语言模型也变得火热。用神经网络训练语言模型的经典之作，要数Bengio等人发表的《A Neural Probabilistic Language Model》，它也是基于N-Gram的，首先将每个单词w_{m-n+1},w_{m-n+2} … w_{m-1}映射到词向量空间，再把各个单词的词向量组合成一个更大的向量作为神经网络输入，输出是P(w_m)。本文将此模型简称为ffnnlm（Feed-forward Neural Net Language Model）。ffnnlm解决了传统n-gram的两个缺陷：(1)词语之间的相似性可以通过词向量来体现；(2)自带平滑功能。

从最新文献看，目前state-of-the-art语言模型应该是基于循环神经网络(recurrent neural network)的语言模型，简称rnnlm。循环神经网络相比于传统前馈神经网络，其特点是：可以存在有向环，将上一次的输出作为本次的输入。而rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n 要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息可以在下一次预测里循环使用。

基于RNN的language model利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的vanishing gradient问题（在RNN里，梯度计算随时间成指数倍增长或衰减，称之为Exponential Error Decay）。所以后来又提出基于LSTM(Long short term memory)的language model，LSTM也是一种RNN网络。LSTM通过网络结构的修改，从而避免vanishing gradient问题。
###自然语言处理中的attention model
NLP里面有一类典型的natural language generation问题：给定一段上下文(context)， 生成一段与context相关的目标文本(target)。典型的例子包括：

机器翻译：context是英文，需要生成对应的中文。

摘要生成：context是新闻内容， 需要生成新闻标题或者摘要。

阅读理解：context是一段文章和一个选择题，需要输出答案。

Deep learning火起来后，最常见的建模方式是用Recurrent Neural Networks (RNN) 将上下文"编码"，然后再"解码"成目标文本。

以机器翻译为例,Google之前的end to end模型中，用一个 RNN encoder读入context， 得到一个context vector（RNN的最后一个hidden state）；然后另一个RNN decoder以这个hidden state为起始state，依次生成target的每一个单词。

这种做法的缺点是，无论之前的context有多长，包含多少信息量，最终都要被压缩成一个几百维的vector。这意味着context越大，最终的state vector会丢失越多的信息。正如楼主贴出blog中的Figure 1所显示，输入sentence长度增加后，最终decoder翻译的结果会显著变差。

事实上，因为context在输入时已知，一个模型完全可以在decode的过程中利用context的全部信息，而不仅仅是最后一个state。

首先，在生成target side的states时 ，所有context vectors都会被当做输入。

其次，并不是所有context都对下一个状态的生成产生影响。例如，当翻译英文文章的时候，我们要关注的是“当前翻译的那个部分”，而不是整篇文章。“Attention”的意思就是选择恰当的context并用它生成下一个状态。

在大部分的论文中，Attention是一个权重vector（通常是softmax的输出），其维度等于context的长度。越大的权重代表对应位置的context越重要。不同论文对attention权重的计算方式不同，但其核心抛不开上述两点。

###自然语言处理中的主题模型（topic model）
在自然语言处理中，主题(topic)可以看成是词项的概率分布。我们使用主题模型对文档的生成过程进行模拟，再通过参数估计得到各个主题。当以词袋( bag of words)形式表示文档时，其维度可能是数万。若指定主题模型的主题个数为K，通过主题模型的训练，最终形成了K个主题，则可以将词项空间中的文档变换到主题空间，得到文档新的表达。由于通常主题的个数K远小于词项的个数，常使用主题模型进行降维。在以文本为处理对象的领域中，降维后的新坐标(即在K个主题上的分量)往往具有语义上的特征。

自2003年提出以来，主题模型逐渐成为机器学习、自然语言处理、机器视觉领域中的重要研宄课题，并且在文本挖掘、观点挖掘、社交网络分析、视频场景理解、蛋白质结构分析、金融数据分析等领域获得了广泛的应用。然而，随着待分析语料库规模越来越大，主题模型分析出的主题数目也越来越多，主题模型产生的结果越来越难以利用。

PLSA和LDA是两种比较流行的主题模型。概率潜语义分析(Probability Latent Semantic Analysis, PLSA)是由hoffmann 在2003年提出的基于潜在语义分析LSA（Latent Semantic Analysis）和概率模型的新的文本挖掘方法。PLSA通过概率方法和生成式模型推导出文本的主题分布。PLSA假设文档隐藏潜在的主题。生成文档时，根据“文档一主题”概率模型随机选择一个主题，然后根据“主题一词语”概率模型随机选择一个词语，不断重复，直到生成全部文本集合。PLSA需要计算“文档一主题”和“主题一词语”两个模型参数，一般采用EM算法进行参数估计。然而PLSA在“文档一主题”概率模型计算上没有使用先验分布，参数过多会导致过拟合现象，并且很难对训练集以外的文档进行概率计算。因此Blei在2003年提出LDA模型，LDA在PLSA的基础上引入了超参数，形成三层的贝叶斯模型。通常采用Gibbs抽样的方法对LDA模型参数进行估计。LDA模型参数是“文档一主题”概率分布和“主题一词语”概率分布。此外，LDA模型能够对非训练集文档之外的主题分布进行计算。

主题模型的用途：

1.计算文本的相似性，考虑到文本语义，更好的刻画文本相似性，避免多义词，同义词的影响

2.文本聚类，用户聚类(RS)

3.去除噪音，只保留最重要的主题，更好的刻画文档

参考文献：自然语言处理中主题模型的发展[J]. 计算机学报,2011,08:1423-1436.

其它参考文献：http://blog.csdn.net/hxxiaopei/article/details/7617838

#### 语义表达中逻辑模糊解读

语义的逻辑研究进程：由于语义在边界状况的适应性不确定对经典二值逻辑提出了挑战,三值逻辑应运而生,办法是在传统二值的基础上引入第三值———不定值 。但由于三值逻辑的不足,而后建立了模糊逻辑：即以无穷多值逻辑来刻画语义适用的边界情况。它采用一个类似模糊集合的区间来进一步准确反映出语义在一定范围内适用的真值度，并准确计算复杂句子的真值度。随着问题复杂性的增加，发现三值逻辑和模糊逻辑都面临一个无法解决的高阶模糊问题（即高阶模糊容易使问题陷于无穷而非常难以解决）,逻辑的研究法受到了质疑。逻辑研究法是为了建立一个抽象、概括的语义模型,关键在于能准确反映语义模糊的本质和规律而并非追求一个精确的值,为应对当前复杂问题，逻辑语义模型仍旧需要进一步改善。

三种语义逻辑研究的优缺点：
1.二值逻辑：
人们对于语词适用的典型区域和不能适用的范围是很清楚的,在这两个范围使用语词也不会犹豫,而对于边界区域,在语词的使用上就会感到犹豫。传统逻辑的规律是排中律(excluded mid dle), 排中律具有二值性:任何命题p,p要么为真,要么为假。根据这个二值逻辑,任何语词的使用也非真即假,在其适用范围内为真,否则为假。
优点：在明显适用领域二值逻辑完全适用。
缺点：在边界区域，难以作出正确决定。
2.三值逻辑：
见上所述，二值逻辑的缺点直接导致了三值逻辑的诞生。引入了一个新值———不确定值,即非真非假,于是对于语词适用的三个区域———典型适用区域、边界区域、非适用区域 ,在逻辑上都有了对应的真值性,即真、不确定和假。
当用符号来表示一些逻辑条件:“ -”表示否定,“&”表示和,“∨”表示或者,“->”表示如果...就,“≡”表示如果...并且只有这样。在分析后二值逻辑条件下,复杂句的取值情况与三值逻辑条件下的情况会有一定程度的不同。在连锁推理悖论的“谷粒堆”问题中，随着谷粒一个一个的增加 , 在二值逻辑条件下,我们无法确定第几个谷粒的增加能使谷粒立刻“成堆”,即“谷粒堆”立刻由假成真。同样的道理,在三值逻辑条件下，我们也无法知道第几个谷粒的增加能使“谷粒堆”由假变为不确定,进而第几个谷粒的增加使之 由不确定变为真。因而真与假、真与不定、不定与假之间都没有明晰的分界,它们之间都是渐次性的过渡。
优点：进一步解决边界区域的逻辑问题。
缺点：引入高阶模糊问题：所谓高阶模糊指的是元语言(meta-language,用来描述自然语言的语言)的模糊。根据三值逻辑,对任意命题p,p可以取不定值,那么“p为不定值 ”的取值又如何呢?是真、是假、还是不定呢?“p为不定值”也可以取不定值,这被称为二级模糊(second-ordervaguenes)。以此类推,还可以出现三级模糊、四级模糊以至于高阶模糊。
3.模糊逻辑：
模糊逻辑通过把真值度的变化视为一个模糊集合的方法,更深入、准确的刻画了事物变化的连续性和渐次性,通过数学方式计算出复杂句的真值度。
优点：与二值逻辑和三值逻辑的方法相比更为深入准确。
缺点：此方法也同样不能解决高阶模糊问题。设有一命题p,p的真值度为0.9,那么“p的真值度为0.9”的真值度又是多少呢?如果是0.8,那么“p的真值度为0.9“的真值度为 0.8”的真值度又是多少呢?如此又陷入了无穷反复,因为不管你在0到1区间取任何值,它可能都不是p的真实的真值度。

鉴于上述三种￼逻辑分析方法：
优点：一步步建立并完善了语义逻辑分析的模型，做到了每一阶段的改进。
缺点：此方法也同样不能解决高阶模糊问题。设有一命题p,p的真值度为0.9,那么“p的真值度为0.9”的真值度又是多少呢?如果是0.8,那么“p的真值度为0.9“的真值度为 0.8”很多人认为从二值逻辑到三值逻辑到模糊逻辑。 ￼
