

#  语义

## 语义相关的英文词汇
semantics ： 语义学

semantic atom  : 语义原子

semantic componentlal analysis : 语义成分分析

semantic field : 语义场

convolutional neural network ：卷积神经网络

## 语义的范畴
在语言中，语义范畴是一个复杂的概念。有的称作语法范畴，有的称作语义语法范畴，还有的称作句法范畴，等等。

一个语义范畴就是一束语言意义的集合，认知范畴通过语言化形成语义范畴，语义范畴仍需借助一定语言形式来表现。而表现语义范畴的语言形式主要有词汇、语法和语用三个方面，也就是说认知范畴是通过词汇、语法和语用来实现语言化。

词汇是语言的基础单位，它包括语素、词、固定短语以及这些单位的构成理据。词语的构成理据反映着语言的“规约性”，往往包含着更多的认知内容和更为复杂的认知过程。这是认知范畴在词汇层面所表现的语义范畴，我们称为“词汇语义范畴”。

语法是组词成句的各种语言规则，认知范畴在语法层面的语言化所形成的语义范畴，往往更为抽象复杂。语法层面既有语法形式和语法意义相对应的关系，还存在语法形式和语法意义不对应的情况，我们也需要将其考虑进来。语义范畴在语法层面的表现是复杂多样的，在这一平面上的语义范畴我们称之为“语法语义范畴”。

语用所关系的是使用语言的人，在言语交际中更多的是考虑到人的因素，因而认知范畴在语用层面的表现则是思维最直接的体现，语篇结构所表现的心理图式更直接地反映
着认知的结果。语用平面所表现的语义范畴我们称之为语用语义范畴。
## 语义分析有哪些
### 文本语义分析
文本处理的基本方法构成了语义分析的基础

一个文本串除了分词，还需要做词性标注，命名实体识别，新词发现等。通常有两种方案，一种是pipeline approaches，就是先分词，再做词性标注；另一种是joint approaches，就是把这些任务用一个模型来完成。
    
#### 文本基本处理

##### 1.1中文分词

* 基于字符串匹配的分词方法。此方法按照不同的扫描方式，逐个查找词库进行分词。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分(即最短路径)；总之就是各种不同的启发规则。
* 全切分方法。它首先切分出与词库匹配的所有可能的词，再运用统计语言模型决定最优的切分结果。它的优点在于可以解决分词中的歧义问题。对于文本串“南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型找到最优路径，最后可能还需要命名实体识别。下图中“南京市 长江 大桥”的语言模型得分，即P(南京市，长江，大桥)最高，则为最优切分。
* 由字构词的分词方法。可以理解为字的分类问题，也就是自然语言处理中的sequence labeling问题，通常做法里利用HMM，MAXENT，MEMM，CRF等预测文本串每个字的tag，譬如B，E，I，S，这四个tag分别表示：beginning, inside, ending, single，也就是一个词的开始，中间，结束，以及单个字的词。 例如“南京市长江大桥”的标注结果可能为：“南(B)京(I)市(E)长(B)江(E)大(B)桥(E)”。由于CRF既可以像最大熵模型一样加各种领域feature，又避免了HMM的齐次马尔科夫假设，所以基于CRF的分词目前是效果最好的。除了HMM，CRF等模型，分词也可以基于深度学习方法来做。

参考文献：[斯坦福课程-语言模型] (http://52opencourse.com/111/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88language-modeling%EF%BC%89)

##### 1.2Term Weighting
对文本分词后，接下来需要对分词后的每个term计算一个权重，重要的term应该给与更高的权重。Term weighting在文本检索，文本相关性，核心词提取等任务中都有重要作用。

利用有监督机器学习方法来预测weight。这里类似于机器学习的分类任务，对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。既然是有监督学习，那么就需要训练数据。如果采用人工标注的话，极大耗费人力，所以可以采用训练数据自提取的方法，利用程序从搜索日志里自动挖掘。从海量日志数据里提取隐含的用户对于term重要性的标注，得到的训练数据将综合亿级用户的“标注结果”，覆盖面更广，且来自于真实搜索数据，训练结果与标注的目标集分布接近，训练数据更精确。

* 从搜索session数据里提取训练数据，用户在一个检索会话中的检索核心意图是不变的，提取出核心意图所对应的term，其重要性就高。
* 从历史短串关系资源库里提取训练数据，短串扩展关系中，一个term出现的次数越多，则越重要。
* 从搜索广告点击日志里提取训练数据，query与bidword共有term的点击率越高，它在query中的重要程度就越高。

参考文献：[Deep learning for Chinese word segmentation and POS tagging]( http://www.aclweb.org/anthology/D13-1061)

[Max-margin tensor neural network for chinese word segmentation]( http://aclweb.org/anthology/P14-1028)

[Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)

[Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)

[Chinese Segmentation and New Word Detection using Conditional Random Fields](http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1091&context=cs_faculty_pubs)

### 对于不同层次的处理对象，都需要语义分析：
 - 对于“词”：
   - 词义消歧：判断一个词是不是多义词，对多义词不同含义的表示和判别，结合上下文判断多义词在当前环境下的意思。（方法：根据词典释义、基于语义定义、利用语料库）
 - 对于“句子”：
   - 语义角色标注：识别句子中各词的作用，特别是围绕句中的“动词”识别句中相关成分（谓词-论元结构）。（方法：利用语料库）
 - 对于“篇章”：
   - 指代消解：确定文章中的代词是指的哪个名词。
   - 篇章语义整合：把篇章片段的语义正确整合，得到篇章的全局意图。（方法：基于句法，利用语料库）
 - 可以看到语义分析常常利用语料库（例如pdtb这样的带有标注的熟语料库），使用包括统计、机器学习等方法。

### 词语语义分析

词语语义分析研究是指确定词语意义，衡量词之间的语义相似度（相关度）。词语的相似性和相关性既有联系又有区别。相似性指词语间的可替代性，而相关性指词语的相关程度。例如：“爱情”和“浪漫”是 两个语义相关的词语，但它们的语义不可替代。“橙子”和“橘子”是语义相似的词语，它们指同类水果，同时也有语义关联。根据研究方法的不同，国内外已有的词语语义相关度或相似度的研究大体可分为两大类：基于词语语义知识规则（如语义词典、知识库、概念本体库等）的词语语义分析和基于统计的词语语义分析。

#### 基于知识规则的词语语义相似（相关）分析

基于知识规则的词语语义分析是一种基于语言学的词汇语义分析的理性主义方法，它利用词语语义知识库中定义好的概念及其之间的上下文关系等逻辑关系，通过计算两个概念在概念体系中的距离来衡量词 语间的语义相似或相关度。

#### 基于统计的词语语义分析

基于统计的词语语义相似（相关）度分析方法是一种经验主义方法，它以代数理论、概率论和统计论等数学理论为基础，建立在可观察的语言事实上。该方法认为：两个词语语义相似或相关，当且仅当它们处于相似或相关的上下文环境中。通过对大规模语料库的统计，该方法将词语的上下文信息作为语义相似或相关分析的主要参照依据。

参考文献：秦春秀, 祝婷, 赵捧未, 等. 自然语言语义分析研究进展[J]. 图书情报工作, 2014, 58(22): 130-137.

### 图像语义分析
图像理解就是对图像的语义解释.它是以图像为对象，知识为核心，研究图像中何位置有何目标、目标场景之间的相互关系、图像是何场景以及如何应用场景的一门科学。图像理解输入的是数据，输出的是知识，属于图像研究领域的高层内容。语义作为知识信息的基本描述载体，能将完整的图像内容转换成可直观理解的类文本语言表达，在图像理解中起着至关重要的作用。

图像理解中的语义分析在应用领域的潜力是巨大的。图像中丰富的语义知识可提供较精确的图像搜索引擎，生成智能的数字图像相册和虚拟世界中的视觉场景描述。同时，在图像理解本体的研究中，可有效形成 “数据-知识”的相互驱动体系，包含有意义的上下文信息和层状结构信息，能更快速、更准确地识别和检测出场景中的特定目标 (如识别出场景中的 “显示器 ”, 根据场景语义知识可自动识别附近的 “键盘” )。

尽管语义分析在图像理解中处于非常重要的位置，但传统的图像分析方法基本上全部回避了语义问题，仅针对纯粹的图像数据进行分析。究其原因主要集中于两方面：1)图像的视觉表达和语义之间很难建立合理关联 , 描述实体间产生巨大的语义鸿沟;  2)语义本身具有表达的多义性和不确定性。目前，越来越多的研究已开始关注上述 “瓶颈 ”, 并致力于有效模型和方法以实现图像理解中的语义表达。

解决图像理解中的语义鸿沟需要建立图像和文本之间的对应关系，解决的思路可大致分为三类。第一条思路侧重于图像本身的研究，通过构建和图像内容相一致的模型或方法，将语义隐式地融入其中，建立 “文本→图像”的有向联系，核心在于如何将语义融于模型和方法中。采用此策略形成的研究成果多集中于生成方式和判别方式中。第二条思路从语义本身的句 法表达和结构关系入手，分析其组成及相互关系，通过建立与之类似的图像视觉元素结构表达，将语义描述和分析方法显式地植入包含句法关系的视觉图中，建立“图像→文本”的有向联系。核心在于如何构建符合语义规则的视觉关系图。第三条思路面向应用,以基于内容的图像检索为核心,增加语义词汇规模，构建多语义多用户多进程的图像检索查询系统。

图像内容的语义分析借鉴文本分析策略。首先需要构建与之相对应的对象，整幅图像对应整篇 文档，而文档中的词汇也需要对应相应的视觉词汇。视觉词汇的获取一般通过对图像信息的显著性分析提取图像的低层特征，低层特征大多从图像数据获取，包括简单的点线面特征和一些特殊的复杂特征再由鲁棒的特征表达方式生成合适的视觉词汇,视觉词汇一般具有高重用性和若干不变特性。

参考文献：
《图像语义分析与理解综述》
http://xueshu.baidu.com/s?wd=paperuri%3A%281a7e0a172268dc92eb03e4afcd5958f1%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Fwww.cnki.com.cn%2FArticle%2FCJFDTotal-MSSB201002010.htm&ie=utf-8&sc_us=8690416712663863870

## 语义如何表达

### 文本表示
#### 1. 传统向量空间模型
(1) 主要步骤
* 将文本的基本语言单位（字、词、词组、短语）抽取，组成特征项，用tn表示
* 将tn按在文本中的重要性给出权重wn
* 将文本抽象为（t1,w1,t2,w2,……,tn,wn）简化为（w1,w2,……,wn）即为文本的向量 空间模型。

(2) 权值wn计算
* 布尔：取值1/0表示该特征是否在文本中出现；
* 词频（TF）：wn用特征在文档中出现的频数表示；
* 文档频率(IDF)：通过该特征词在所有文档中的出现频率来分配权重；
* TF/IDF权值：将前两种词的特征相乘得到权重值。

传统向量空间模型有明显的缺点：
* 基于关键字的文档处理方法，依据的是词频信息，两个文档的相似度取决于共同词汇的数量，无法分辨自然语言的语义模糊性。
* 假设词与词之间是相互独立的，一个关键字唯一代表一个概念或语义单元，而实际情况是文档存在很多的一词多义和同义词现象，因此这种假设很难满足实际情况。
* 文档中词与词往往存在一定的关联性，信息检索的本质就是语义的检索，孤立的用关键字来表示文档内容，通过简单的词汇模式匹配进行检索，忽视上下文语境的影响作用，会影响到信息检索的结果的查准率和查全率。

#### 2.	one-hot词向量
NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。

举个栗子，
“话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...]

“麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]

每个词都是茫茫 0 海中的一个 1。

这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 ID。比如刚才的例子中，话筒记为 3，麦克记为 8（假设从 0 开始记）。如果要编程实现的话，用 Hash 表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。

#### 3. Distributed representation 词向量
现在俗称的词向量主要指Distributioned Representation，是是一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, ...]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的。

Distributed representation 最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。

Word2vec是比较常用的一种，其基本思想是通过训练将每个词映射成 K 维实数向量（K 一般为模型中的超参数），通过词之间的距离（比如 cosine 相似度、欧氏距离等）来判断它们之间的语义相似度.其采用一个三层的神经网络 ，输入层-隐层-输出层。有个核心的技术是 根据词频用Huffman编码 ，使得所有词频相似的词隐藏层激活的内容基本一致，出现频率越高的词语，他们激活的隐藏层数目越少，这样有效的降低了计算的复杂度。而Word2vec大受欢迎的一个原因正是其高效性，Mikolov 在论文中指出，一个优化的单机版本一天可训练上千亿词。这个三层神经网络本身是对语言模型进行建模，但也同时获得一种单词在向量空间上的表示 ，而这个副作用才是Word2vec的真正目标。
  
Word2Vec实际上是两种不同的方法：Continuous Bag of Words (CBOW) 和 Skip-gram。CBOW的目标是根据上下文来预测当前词语的概率。Skip-gram刚好相反：根据当前词语来预测上下文的概率。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机 N 维向量。经过训练之后，该算法利用 CBOW 或者 Skip-gram 的方法获得了每个单词的最优向量。

#### 4. 文档向量
即使上述模型对词向量进行平均处理，我们仍然忽略了单词之间的排列顺序对情感分析的影响。即上述的word2vec只是基于词的维度进行"语义分析"的，而并不具有上下文的"语义分析"能力。作为一个处理可变长度文本的总结性方法，Quoc Le 和 Tomas Mikolov 提出了 Doc2Vec方法。除了增加一个段落向量以外，这个方法几乎等同于 Word2Vec。和 Word2Vec 一样，该模型也存在两种方法：Distributed Memory(DM) 和 Distributed Bag of Words(DBOW)。DM 试图在给定上下文和段落向量的情况下预测单词的概率。在一个句子或者文档的训练过程中，段落 ID 保持不变，共享着同一个段落向量。DBOW 则在仅给定段落向量的情况下预测段落中一组随机单词的概率。

word2vec&doc2vec具体训练过程参考：[文本深度表示模型——word2vec&doc2vec词向量模型](http://www.wtoutiao.com/p/238FL4B.html)

其它参考文献：

* 文本表示+向量表示 http://blog.sina.com.cn/s/blog_795b865e0102v984.html
* Deep Learning in NLP （一）词向量和语言模型 http://blog.csdn.net/zhoubl668/article/details/23271225 

## 词向量，句向量

### 词向量是什么

  在文本分析的vector space model中，是用向量来描述一个词的，譬如最常见的One-hot representation。One-hot representation方法的一个明显的缺点是，词与词之间没有建立关联。在深度学习中，一般用Distributed Representation来描述一个词，常被称为“Word Representation”或“Word Embedding”，也就是我们俗称的“词向量”。

  词向量起源于hinton在1986年的论文[1]，后来在Bengio的ffnnlm论文[2]中，被发扬光大，但它真正被我们所熟知，应该是word2vec[3]的开源。在ffnnlm中，词向量是训练语言模型的一个副产品，不过在word2vec里，是专门来训练词向量，所以word2vec相比于ffnnlm的区别主要体现在：

  模型更加简单，去掉了ffnnlm中的隐藏层，并去掉了输入层跳过隐藏层直接到输出层的连接。
  训练语言模型是利用第m个词的前n个词预测第m个词，而训练词向量是用其前后各n个词来预测第m个词，这样做真正利用了上下文来预测。

  word2vec的两种训练算法：CBOW(continuous bag-of-words)和Skip-gram。在cbow方法里，训练目标是给定一个word的context，预测word的概率；在skip-gram方法里，训练目标则是给定一个word，预测word的context的概率。

  关于word2vec，在算法上还有较多可以学习的地方，例如利用huffman编码做层次softmax，negative sampling，工程上也有很多trick，具体请参考文章[4][5]。

### 词向量的应用

#### 词向量的应用点：

可以挖掘词之间的关系，譬如同义词。
  可以将词向量作为特征应用到其他机器学习任务中，例如作为文本分类的feature，Ronan collobert在Senna[6]中将词向量用于POS, CHK, NER等任务。
  用于机器翻译[7]。分别训练两种语言的词向量，再通过词向量空间中的矩阵变换，将一种语言转变成另一种语言。
  word analogy，即已知a之于b犹如c之于d，现在给出 a、b、c，C(a)-C(b)+C(c)约等于C(d)，C(*)表示词向量。可以利用这个特性，提取词语之间的层次关系。
  Connecting Images and Sentences，image understanding。例如文献，DeViSE: A deep visual-semantic em-bedding model。
  Entity completion in Incomplete Knowledge bases or ontologies，即relational extraction。Reasoning with neural tensor net- works for knowledge base completion。
  
  除了产生词向量，word2vec还有很多其他应用领域，对此我们需要把握两个概念：doc和word。在词向量训练中，doc指的是一篇篇文章，word就是文章中的词。

  假设我们将一簇簇相似的用户作为doc（譬如QQ群），将单个用户作为word，我们则可以训练user distributed representation，可以借此挖掘相似用户。
  假设我们将一个个query session作为doc，将query作为word，我们则可以训练query distributed representation，挖掘相似query。

### 句向量

  分析完word distributed representation，我们也许会问，phrase，sentence是否也有其distributed representation。最直观的思路，对于phrase和sentence，我们将组成它们的所有word对应的词向量加起来，作为短语向量，句向量。在参考文献中，验证了将词向量加起来的确是一个有效的方法，但事实上还有更好的做法。

  Le和Mikolov在文章《Distributed Representations of Sentences and Documents》[8]里介绍了sentence vector，这里我们也做下简要分析。

  先看c-bow方法，相比于word2vec的c-bow模型，区别点有：

  训练过程中新增了paragraph id，即训练语料中每个句子都有一个唯一的id。paragraph id和普通的word一样，也是先映射成一个向量，即paragraph vector。paragraph vector与word vector的维数虽一样，但是来自于两个不同的向量空间。在之后的计算里，paragraph vector和word vector累加或者连接起来，作为输出层softmax的输入。在一个句子或者文档的训练过程中，paragraph id保持不变，共享着同一个paragraph vector，相当于每次在预测单词的概率时，都利用了整个句子的语义。
  在预测阶段，给待预测的句子新分配一个paragraph id，词向量和输出层softmax的参数保持训练阶段得到的参数不变，重新利用梯度下降训练待预测的句子。待收敛后，即得到待预测句子的paragraph vector。

### 词向量的改进

  学习词向量的方法主要分为：Global matrix factorization和Shallow Window-Based。Global matrix factorization方法主要利用了全局词共现，例如LSA；Shallow Window-Based方法则主要基于local context window，即局部词共现，word2vec是其中的代表；Jeffrey Pennington在word2vec之后提出了GloVe，它声称结合了上述两种方法，提升了词向量的学习效果。
  目前通过词向量可以充分发掘出“一义多词”的情况，譬如“快递”与“速递”；但对于“一词多义”，束手无策，譬如“苹果”(既可以表示苹果手机、电脑，又可以表示水果)，此时我们需要用多个词向量来表示多义词。

参考文献：
[1] Learning distributed representations of concepts
 http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs202_sp12/Readings/hinton86.pdf
[2] A neural probabilistic language model 2003 http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
[3] word2vec https://code.google.com/p/word2vec/
[4] Deep Learning实战之word2vec http://techblog.youdao.com/?p=915
[5] word2vec中的数学原理详解 http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178
[6] Senna http://ml.nec-labs.com/senna
[7] Exploting similarities among language for machine translation http://arxiv.org/pdf/1309.4168.pdf
[8] Distributed Representations of Sentences and Documents http://arxiv.org/pdf/1405.4053v2.pdf

#语言模型

语言模型是用来计算一个句子产生概率的概率模型，即P(w_1,w_2,w_3…w_m)，m表示词的总个数。根据贝叶斯公式：P(w_1,w_2,w_3 … w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2) … P(w_m|w_1,w_2 … w_{m-1})。

最简单的语言模型是N-Gram，它利用马尔科夫假设，认为句子中每个单词只与其前n–1个单词有关，即假设产生w_m这个词的条件概率只依赖于前n–1个词，则有P(w_m|w_1,w_2…w_{m-1}) = P(w_m|w_{m-n+1},w_{m-n+2} … w_{m-1})。其中n越大，模型可区别性越强，n越小，模型可靠性越高。

N-Gram语言模型简单有效，但是它只考虑了词的位置关系，没有考虑词之间的相似度，词语法和词语义，并且还存在数据稀疏的问题，所以后来，又逐渐提出更多的语言模型，例如Class-based ngram model，topic-based ngram model，cache-based ngram model，skipping ngram model，指数语言模型（最大熵模型，条件随机域模型）等。

最近，随着深度学习的兴起，神经网络语言模型也变得火热。用神经网络训练语言模型的经典之作，要数Bengio等人发表的《A Neural Probabilistic Language Model》，它也是基于N-Gram的，首先将每个单词w_{m-n+1},w_{m-n+2} … w_{m-1}映射到词向量空间，再把各个单词的词向量组合成一个更大的向量作为神经网络输入，输出是P(w_m)。本文将此模型简称为ffnnlm（Feed-forward Neural Net Language Model）。ffnnlm解决了传统n-gram的两个缺陷：(1)词语之间的相似性可以通过词向量来体现；(2)自带平滑功能。

从最新文献看，目前state-of-the-art语言模型应该是基于循环神经网络(recurrent neural network)的语言模型，简称rnnlm。循环神经网络相比于传统前馈神经网络，其特点是：可以存在有向环，将上一次的输出作为本次的输入。而rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n 要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息可以在下一次预测里循环使用。

基于RNN的language model利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的vanishing gradient问题（在RNN里，梯度计算随时间成指数倍增长或衰减，称之为Exponential Error Decay）。所以后来又提出基于LSTM(Long short term memory)的language model，LSTM也是一种RNN网络。LSTM通过网络结构的修改，从而避免vanishing gradient问题。
###自然语言处理中的attention model
NLP里面有一类典型的natural language generation问题：给定一段上下文(context)， 生成一段与context相关的目标文本(target)。典型的例子包括：

机器翻译：context是英文，需要生成对应的中文。

摘要生成：context是新闻内容， 需要生成新闻标题或者摘要。

阅读理解：context是一段文章和一个选择题，需要输出答案。

Deep learning火起来后，最常见的建模方式是用Recurrent Neural Networks (RNN) 将上下文"编码"，然后再"解码"成目标文本。

以机器翻译为例,Google之前的end to end模型中，用一个 RNN encoder读入context， 得到一个context vector（RNN的最后一个hidden state）；然后另一个RNN decoder以这个hidden state为起始state，依次生成target的每一个单词。

这种做法的缺点是，无论之前的context有多长，包含多少信息量，最终都要被压缩成一个几百维的vector。这意味着context越大，最终的context vector会丢失越多的信息。通常输入sentence长度增加后，最终decoder翻译的结果会显著变差。

事实上，并不是所有context都对下一个状态的生成产生影响。例如，当翻译英文文章的时候，我们要关注的是“当前翻译的那个部分”，而不是整篇文章。“Attention”的意思就是选择恰当的context并用它生成下一个状态。

在大部分的论文中，Attention是一个权重vector（通常是softmax的输出），其维度等于context的长度。越大的权重代表对应位置的context越重要。不同论文对attention权重的计算方式不同，但其核心抛不开上述两点。

###自然语言处理中的主题模型（topic model）
在自然语言处理中，主题(topic)可以看成是词项的概率分布。我们使用主题模型对文档的生成过程进行模拟，再通过参数估计得到各个主题。当以词袋( bag of words)形式表示文档时，其维度可能是数万。若指定主题模型的主题个数为K，通过主题模型的训练，最终形成了K个主题，则可以将词项空间中的文档变换到主题空间，得到文档新的表达。由于通常主题的个数K远小于词项的个数，常使用主题模型进行降维。在以文本为处理对象的领域中，降维后的新坐标(即在K个主题上的分量)往往具有语义上的特征。

自2003年提出以来，主题模型逐渐成为机器学习、自然语言处理、机器视觉领域中的重要研宄课题，并且在文本挖掘、观点挖掘、社交网络分析、视频场景理解、蛋白质结构分析、金融数据分析等领域获得了广泛的应用。然而，随着待分析语料库规模越来越大，主题模型分析出的主题数目也越来越多，主题模型产生的结果越来越难以利用。

PLSA和LDA是两种比较流行的主题模型。概率潜语义分析(Probability Latent Semantic Analysis, PLSA)是由hoffmann 在2003年提出的基于潜在语义分析LSA（Latent Semantic Analysis）和概率模型的新的文本挖掘方法。PLSA通过概率方法和生成式模型推导出文本的主题分布。PLSA假设文档隐藏潜在的主题。生成文档时，根据“文档一主题”概率模型随机选择一个主题，然后根据“主题一词语”概率模型随机选择一个词语，不断重复，直到生成全部文本集合。PLSA需要计算“文档一主题”和“主题一词语”两个模型参数，一般采用EM算法进行参数估计。然而PLSA在“文档一主题”概率模型计算上没有使用先验分布，参数过多会导致过拟合现象，并且很难对训练集以外的文档进行概率计算。因此Blei在2003年提出LDA模型，LDA在PLSA的基础上引入了超参数，形成三层的贝叶斯模型。通常采用Gibbs抽样的方法对LDA模型参数进行估计。LDA模型参数是“文档一主题”概率分布和“主题一词语”概率分布。此外，LDA模型能够对非训练集文档之外的主题分布进行计算。

主题模型的用途：

1.计算文本的相似性，考虑到文本语义，更好的刻画文本相似性，避免多义词，同义词的影响


2.文本聚类，用户聚类(RS)

3.去除噪音，只保留最重要的主题，更好的刻画文档

参考文献：自然语言处理中主题模型的发展[J]. 计算机学报,2011,08:1423-1436.

其它参考文献：http://blog.csdn.net/hxxiaopei/article/details/7617838

#### 语义表达中逻辑模糊解读

语义的逻辑研究进程：由于语义在边界状况的适应性不确定对经典二值逻辑提出了挑战,三值逻辑应运而生,办法是在传统二值的基础上引入第三值———不定值 。但由于三值逻辑的不足,而后建立了模糊逻辑：即以无穷多值逻辑来刻画语义适用的边界情况。它采用一个类似模糊集合的区间来进一步准确反映出语义在一定范围内适用的真值度，并准确计算复杂句子的真值度。随着问题复杂性的增加，发现三值逻辑和模糊逻辑都面临一个无法解决的高阶模糊问题（即高阶模糊容易使问题陷于无穷而非常难以解决）,逻辑的研究法受到了质疑。逻辑研究法是为了建立一个抽象、概括的语义模型,关键在于能准确反映语义模糊的本质和规律而并非追求一个精确的值,为应对当前复杂问题，逻辑语义模型仍旧需要进一步改善。

三种语义逻辑研究的优缺点：
1.二值逻辑：
人们对于语词适用的典型区域和不能适用的范围是很清楚的,在这两个范围使用语词也不会犹豫,而对于边界区域,在语词的使用上就会感到犹豫。传统逻辑的规律是排中律(excluded mid dle), 排中律具有二值性:任何命题p,p要么为真,要么为假。根据这个二值逻辑,任何语词的使用也非真即假,在其适用范围内为真,否则为假。
优点：在明显适用领域二值逻辑完全适用。
缺点：在边界区域，难以作出正确决定。
2.三值逻辑：
见上所述，二值逻辑的缺点直接导致了三值逻辑的诞生。引入了一个新值———不确定值,即非真非假,于是对于语词适用的三个区域———典型适用区域、边界区域、非适用区域 ,在逻辑上都有了对应的真值性,即真、不确定和假。
当用符号来表示一些逻辑条件:“ -”表示否定,“&”表示和,“∨”表示或者,“->”表示如果...就,“≡”表示如果...并且只有这样。在分析后二值逻辑条件下,复杂句的取值情况与三值逻辑条件下的情况会有一定程度的不同。在连锁推理悖论的“谷粒堆”问题中，随着谷粒一个一个的增加 , 在二值逻辑条件下,我们无法确定第几个谷粒的增加能使谷粒立刻“成堆”,即“谷粒堆”立刻由假成真。同样的道理,在三值逻辑条件下，我们也无法知道第几个谷粒的增加能使“谷粒堆”由假变为不确定,进而第几个谷粒的增加使之 由不确定变为真。因而真与假、真与不定、不定与假之间都没有明晰的分界,它们之间都是渐次性的过渡。
优点：进一步解决边界区域的逻辑问题。缺点：引入高阶模糊问题：所谓高阶模糊指的是元语言(meta-language,用来描述自然语言的语言)的模糊。根据三值逻辑,对任意命题p,p可以取不定值,那么“p为不定值 ”的取值又如何呢?是真、是假、还是不定呢?“p为不定值”也可以取不定值,这被称为二级模糊(second-ordervaguenes)。以此类推,还可以出现三级模糊、四级模糊以至于高阶模糊。
3.模糊逻辑：
模糊逻辑通过把真值度的变化视为一个模糊集合的方法,更深入、准确的刻画了事物变化的连续性和渐次性,通过数学方式计算出复杂句的真值度。
优点：与二值逻辑和三值逻辑的方法相比更为深入准确。
缺点：此方法也同样不能解决高阶模糊问题。设有一命题p,p的真值度为0.9,那么“p的真值度为0.9”的真值度又是多少呢?如果是0.8,那么“p的真值度为0.9“的真值度为 0.8”的真值度又是多少呢?如此又陷入了无穷反复,因为不管你在0到1区间取任何值,它可能都不是p的真实的真值度。

鉴于上述三种￼逻辑分析方法：
优点：一步步建立并完善了语义逻辑分析的模型，做到了每一阶段的改进。
缺点：此方法也同样不能解决高阶模糊问题。设有一命题p,p的真值度为0.9,那么“p的真值度为0.9”的真值度又是多少呢?如果是0.8,那么“p的真值度为0.9“的真值度为 0.8”很多人认为从二值逻辑到三值逻辑到模糊逻辑。 ￼


### 语义角色标注
####概念

语义角色标注是一种浅层语义分析技术，它以句子为单位，不对句子所包含的语义信息进行深入分析，而只是分析句子中的谓词-论元结构。具体，语义角色分析的任务是以句子的谓词为中心，研究句子中各成分与谓语之间的关系，并用语义角色来标注他们的语义角色标注所面临的问题主要体现在鲁棒性。
面向微博搜索的时间感知的混合语言模型
时间是影响信息检索特别是微博检索的重要因素。现有的代表性工作是将时间信息作为文档先验融入统计语言检索模型，目前主要有跟查询无关和跟查询有关两种做法。这两种做法 得到的模型均基于“ 时间越新文档越重要” 这个简单假设。然而，对实际数据集进行分析发现，大多数微博查询的大部分相关文档并没有出现在最新时刻，因此上述假设并不成立。从这一点出发，定义这些相关文档集中出现的高峰点为热门时刻（Hot Time）， 并提出新假设“ 越靠近热门时刻， 文档越重要”。基于该假设，提出基于热门时刻的4个系列模型（HTLMs）。在此基础上，将查询无关模型看作是文档的背景时间信息而将查询有关模型看作是文档的独立时间信息，由此引入平滑思想提出混合的时间模型（MTLM）。
热门时刻（ Hot Time）： 给定一个查询和查询的相关文档集合， 统计每个时刻的文档出现数目， 文档数目突出的时刻称为查询热门时刻。在给定查询条件下， 时间越靠近查
询的热门时刻， 文档越重要．
基于查询热门时刻的语言模型（HTLM）
假定时间越靠近查询的热门时刻，文档越重要。那么当无法得到查询的相关文档集合时，如何找到查询热门时刻。对此，需要分析查询的伪相关反馈文档集（ 检索返回结果的前 Ｎ 篇文档）的时间分布，观察其与真实相关文档时间分布之间的关系。发现有较大比例的查询相关文档和伪相关文档的最热门时刻相当接近。因此可以用伪相关反馈文档集合的时间分布来近似真实相关分布。基于伪相关反馈集合，计算文档的先验概率P(D)。文档离伪相关文档最热门时刻越近， 其先验应该越大。
混合时间语言模型（ MTLM）
根据是否引入查询信息， 可以将已有工作分为两大类：（1）与查询无关的方法，如前面介绍的LC；（2）与查询有关的方法，EGLM以及上面所说的HTLM都属于此类。我们将第１类记为P(DT)，第２类记为P(DQT)。跟查询无关的算法，就是在整体背景下， 定义文档和时间的关系，Li和 Croft认为时间越新文档越重要，这部分信息可以看作是文档的背景信息。跟查询有关的算法，也就是在定义文档和时间的关系时，需要考虑当前查询的特性，同一时间刻度在查询不同时，所代表的时间先验也是不同的。对于一篇文档而言，背景时间信息和基于查询的时间信息都很重要。只运用背景信息，所有文档都千篇一律，不能进行有效的区分。而只运用查询特性信息，该数据集的本身时间特性会被忽略，可能会导致查询特性信息过于突出而影响结果，因此，结合平滑思想，提出一个混合时间语言模型（ Mixed Time Language Model，MTLM）。

* 标注的效果过于依赖于句法分析的效果* 角色标注方法的领域适应性太差
####语义角色标注的基本方法
* 基于短语结构树的语义角色标注* 基于浅层句法分析的语义角色标注
* 基于依存句法分析的角色标注
* 语义角色标注的融合方法

####语义角色标注的基本流程
1. 句法分析结果
2. 候选论元剪除
3. 论元辨识4. 论元标注
5. 后处理6. 语义角色标注结果

参考资料：《统计自然语言处理.宗成庆》


### 基于语义分析的情感挖掘
细粒度的文本情感挖掘是近年来数据挖掘和自然语言处理领域的热门研究，它需要挖掘出文本中每个评价对象的情感倾向性。随着电子商务飞速发展，商品评论中的情感倾向性分析逐渐成为当前的研究热点。它的研究目的是利用网络上丰富的顾客评论资源进行商品的市场反馈分析，为生产商和消费者提供直观的针对商品各个特性的网络评价报告。不仅如此，情感分析还可以对网络上的各种信息，特别是主观性文本，进行倾向性分析，从而更好地理解用户的消费习惯，分析热点事件的舆情，为企业、政府等机构提供重要的决策依据。

#### 情感词典与情感语料库
情感词是研究文本情感分析的基础。情感词典研究主要分为情感词获取、情感词极性判定及量化计算、情感词的存储管理等工作。现阶段的情感词获取工作主要通过现有情感词典及语料库资源进行抽取，或者利用这些资源直接派生出面向具体应用的情感词典。极性强度量化计算是对情感词的情感倾向值进行计算，由于情感词的情感倾向值对上层应用中的情感分析、情感统计工作影响越来越大，所以目前也吸引了不少学者进行研究。总的来说，目前的情感词典研究更多集中在情感词的褒贬分类上，而其情感倾向值往往通过人工标注得出，量化计算方法研究相对缺乏。

文本情感语料库的建设相比情感词典起步较晚。由于情感分析任务的特殊性，现有的文本情感语料库基本都是面向领域的。领域语料库有原始语料素材库的标注语料库。语料库的标注程度和精确度直接影响情感倾向分析结果的准确度。

#### 文本主观性分类
在通常的网络文本中，存在大量的客观性文本和主观性文本。客观性文本是一种不带有感情色彩的对个人、事物或事件的一种客观性描述，而主观性文本主要描述作者对事物、人物、事件等的想法或看法。主观性文本是文本情感分析的主要对象，因此对大量的网络文本事先进行主客观文本识别非常重要，这能够有效地缩小分析范围、减少干扰。主观性文本识别主要以情感词为主，利用各种文本特征表示方法和分类器进行分类识别，其根本问题在于分类特征的选取。

参考：[基于语义分析的评价对象-情感词对抽取](http://www.cnki.net/KCMS/detail/11.1826.tp.20160124.2010.008.html?uid=WEEvREcwSlJHSldRa1FhcEE0L01SbTNuU1V6NUVRekdXK2JUZEthNEVCOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MDQ4MDNCZHJHNEg5Zk1ybzFCWk9zTVl3OU16bVJuNmo1N1QzZmxxV00wQ0xMN1I3cWVZZWR2RnlIbFc3cklJMTA9THo3)
[细粒度情感分析研究](http://www.cnki.net/KCMS/detail/detail.aspx?QueryID=8&CurRec=8&recid=&filename=1013230177.nh&dbname=CDFD1214&dbcode=CDFD&pr=&urlid=&yx=&uid=WEEvREcwSlJHSldRa1FhcEE0L01SbTNuU1V6NUVRekdXK2JUZEthNEVCOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MjYwODhaK2RuRnlEblc3ck5WRjI2SGJHN0h0RExxSkViUElSOGVYMUx1eFlTN0RoMVQzcVRyV00xRnJDVVJMeWU=)

###基于修饰关系的自然语言语义分析方法 
 当前在概念、关系层次上的语义分析方法主要有两种：基于统计的特征向量抽取方法和基于语义词典(WordNet、HowNet等)的语义相似度计算方法。对于具体应用这两种方法都具有较大不足，前者由于统计模型的关系只适用于段落、篇章或多文档等粗粒度的语义分析，而不适合在句子词汇一级的应用；后者能方便处理实体概念之间的各种关系，但是如果想正确处理真实文本中的复杂修饰关系如概念与事件、概念与概念修饰、事件与事件修饰等关系，还需对语义词典和计算方法做进一步的扩展。

 语句是自然语言中最重要的结构单位，语句分析是自然语言处理的核心内容，语句分析最终是为了确定语句的语义，以达到理解语句的目的。语句语义是指句中每个词的意义及词之间的相互关系。语句分析通常的做法是按照词法分析、语法分析和语义分析的步骤来处理。词法分析的任务是把句中的词分离出来，并给每个词指派一个合适的词性；语法分析的任务是在词法分析的基础上进一步确定句法成分，如名词短语、动词短语、小句等，然后判断每个短语的句法功能，如主语、谓语、宾语等；语义分析的任务是确定语义角色，并最终得到句子的意义表示。

 按照真实句子中词之间的修饰关系建立知识库后，在修饰关系分析框架下，修饰关系知识库和构句法是基础。通过词在修饰关系知识库中所处的位置可以确定语句中单个词的含义；通过对知识库中已有的修饰关系的计算可以确定语句中词和词之间的修饰关系。由于假设待分析的语句是符合构句法的，因此在分析过程中可以按照构句法对语句逐步归约，以减少词之间修饰关系的判断次数。句法是指在语言形成过程中约定俗成的将句子成分连成合法语句的规则。语言学已总结出的构句法则通常是从词性角度来考虑的。自然语言中存在一些词如“虽然”、“因为”等，连接一个句子，不充当句子成分，但有明确固定的语义。另外一些词如形容词、副词尾“的”、“地”或有语义的标点符号，在句中起调整语句结构和辅助语义作用。在分析包含这些词或符号的句子时，首先将该类词进行语义标记后从原句中略去，然后再进行语句分析。

修饰关系的类型：根据真实文本中出现的实词之间的各种关系，并考虑表意虚词在句中的结构支配作用，将词间的修饰关系概括为以下16种关系类型。
a.动作(主体词和动作词之间的关系，修饰方向由动作词指向主体词)；
b.变化(主体词和变化词之间的关系，修饰方向由变化词指向主体词)；
c.述态(主体词和述态词之间的关系，修饰方向由述态词指向主体词)；
d.情态(述态词和情态词之间的关系，修饰方向由情态词指向述态词)；
e.修饰(目标词和修饰词之间的关系，目标词可为动、名形容词等词性，修饰方向由修饰词指向目标词)；
f．限定(目标词和限定词之间的关系，目标词可为动、名、形容词等词性，修饰方向由限定词指向目标词)；
g.度量(目标词和度量词之间的关系，目标词为名词性，修饰方向由度量词指向目标词)；
h.数目(数目词和度量词之间的关系，修饰方向由数目词指向目标词)；
i．时间(时间词和动作词之间的关系，修饰方向由时间词指向动作词)；
j．空间(空间词和动作词之间的关系，修饰方向由空间词指向动作词)；
k.方式(方式词和动作词之间的关系，方式词可为介、副词等词性，修饰方向由方式词指向动作词)；
L.工具(工具词和动作词之间的关系，工具词可为介词、副词等词性，修饰方向由工具词指向动作词)；
m.并列(目标词和并列词之间的关系，并列词通常为连词词性，修饰方向由并列词指向目标词)；
n.陈述状态(状态词和述态词之间的关系，状态词可为名、介、形容词等词性。修饰方向由状态词指向述态词)；
o．动变对象(对象词和动作、变化词之间的关系，修饰方向由对象词指向动作、变化词)；．
p．介词对象(对象词和作为时间、空间、方式、工具、陈述状态等关系中介词之间的关系，由对象词指向介词)

  基于修饰关系的语句分析由以下几个步骤完成：
  构建知识库。修饰关系知识库是对语句进行分析的基础，本文通过对真实文本语料库中的语句进行指导分析，将分析得到的修饰关系保存到知识库中。
  分析预处理。该过程主要完成两部分任务：一是分词(将待句子中的词语用空格分开)，二是按照预先定义好的语义范式对句子进行语义标记。从理论上，本文提出的分析方法在修饰关系足够多的情况下不需要预先进行分词处理。在做测试时，分词工作是使用北京大学语言研究所的分词系统结合部分手工操作完成的。
   计算句子语义量。从左向右扫描句子，依次按照构句法确定句中的需要计算的两词，并根据知识库中的修饰关系计算两词之间是否有某种修饰关系，并确定其词语相关度。最后根据词语相关度计算句子语义量。
   确定分析结果。从多个可能的分析中选择语义量最小的分析作为最终分析结果。通过分析结果可以确定句子的主、谓、宾等常规成分。
   更新关系库。将句子分析出的修饰关系增加到修饰关系库中，这样在分析的过程中关系库可以得到扩充。

###面向微博搜索的时间感知的混合语言模型

时间是影响信息检索特别是微博检索的重要因素。现有的代表性工作是将时间信息作为文档先验融入统计语言检索模型，目前主要有跟查询无关和跟查询有关两种做法。这两种做法 得到的模型均基于“ 时间越新文档越重要” 这个简单假设。然而，对实际数据集进行分析发现，大多数微博查询的大部分相关文档并没有出现在最新时刻，因此上述假设并不成立。从这一点出发，定义这些相关文档集中出现的高峰点为热门时刻（Hot Time）， 并提出新假设“ 越靠近热门时刻， 文档越重要”。基于该假设，提出基于热门时刻的4个系列模型（HTLMs）。在此基础上，将查询无关模型看作是文档的背景时间信息而将查询有关模型看作是文档的独立时间信息，由此引入平滑思想提出混合的时间模型（MTLM）。

热门时刻（ Hot Time）： 给定一个查询和查询的相关文档集合， 统计每个时刻的文档出现数目， 文档数目突出的时刻称为查询热门时刻。在给定查询条件下， 时间越靠近查询的热门时刻，文档越重要．

####基于查询热门时刻的语言模型（HTLM）
假定时间越靠近查询的热门时刻，文档越重要。那么当无法得到查询的相关文档集合时，如何找到查询热门时刻。对此，需要分析查询的伪相关反馈文档集（ 检索返回结果的前 Ｎ 篇文档）的时间分布，观察其与真实相关文档时间分布之间的关系。发现有较大比例的查询相关文档和伪相关文档的最热门时刻相当接近。因此可以用伪相关反馈文档集合的时间分布来近似真实相关分布。基于伪相关反馈集合，计算文档的先验概率P(D)。文档离伪相关文档最热门时刻越近， 其先验应该越大。

####混合时间语言模型（ MTLM）
根据是否引入查询信息， 可以将已有工作分为两大类：（1）与查询无关的方法，如前面介绍的LC；（2）与查询有关的方法，EGLM以及上面所说的HTLM都属于此类。我们将第１类记为P(DT)，第２类记为P(DQT)。跟查询无关的算法，就是在整体背景下， 定义文档和时间的关系，Li和 Croft认为时间越新文档越重要，这部分信息可以看作是文档的背景信息。跟查询有关的算法，也就是在定义文档和时间的关系时，需要考虑当前查询的特性，同一时间刻度在查询不同时，所代表的时间先验也是不同的。对于一篇文档而言，背景时间信息和基于查询的时间信息都很重要。只运用背景信息，所有文档都千篇一律，不能进行有效的区分。而只运用查询特性信息，该数据集的本身时间特性会被忽略，可能会导致查询特性信息过于突出而影响结果，因此，结合平滑思想，提出一个混合时间语言模型（ Mixed Time Language Model，MTLM）。

参考文献：《面向微博搜索的时间感知的混合语言模型》

###常用的关于语义分析的工具
1.Python 文本挖掘：使用scikit-learn 机器学习包进行文本分类 ；简单的自然语言统计 主要使用NLTK （Natural Language Toolkit）程序包；使用机器学习方法进行情感分析
2.腾讯文智：腾讯文智是一个语义分析开放平台，它基于并行计算和分布式爬虫系统，给用户提供语义分析OpenAPI。用户通过短短几行代码即可调用文智API，实现语义分析功能。文智的主要应用场景包括舆情监控、个性化推荐、语义搜索等
3.词典资源：；SentiWordNet；《知网》中文版；中文情感极性词典 NTUSD；情感词汇本体下载
4.自然语言处理工具和平台：哈尔滨工业大学社会计算与信息检索研究中心；isnowfy/snownlp · GitHub
5.汉语分词：自然语言处理与信息检索共享平台 NLPIR.org；fxsjy/jieba · GitHub


